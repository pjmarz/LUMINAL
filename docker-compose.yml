---
# ===========================================================================
# LUMINAL DOCKER COMPOSE CONFIGURATION
# ===========================================================================
# Self-Hosted AI Automation Platform
# This orchestration file defines all LUMINAL services for AI workflows,
# LLM inference, and automation. Services include n8n, Ollama,
# OpenWebUI, Home Assistant, and Qdrant with GPU acceleration support.
# ===========================================================================

name: luminal

services:
  # ==== N8N ====
  # Workflow automation platform with custom AI nodes
  # Provides visual workflow builder for creating automated processes
  n8n:
    image: n8nio/n8n:latest
    hostname: n8n
    container_name: n8n
    networks: [ 'luminal_default' ]
    restart: unless-stopped
    environment:
      - GENERIC_TIMEZONE=${TZ}
      - TZ=${TZ}
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=${N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS:-true}
      - N8N_RUNNERS_ENABLED=${N8N_RUNNERS_ENABLED:-true}
      - N8N_SECURE_COOKIE=${N8N_SECURE_COOKIE:-false} # Disable secure cookies for local development
      - N8N_EDITOR_BASE_URL=http://${N8N_HOST_IP}:${N8N_PORT:-5678}
      - N8N_HOST=0.0.0.0
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://${N8N_HOST_IP}:${N8N_PORT:-5678}/
      - N8N_ENCRYPTION_KEY_FILE=/run/secrets/n8n_encryption_key
      - N8N_USER_MANAGEMENT_JWT_SECRET_FILE=/run/secrets/n8n_jwt_secret
    ports:
      - ${N8N_PORT:-5678}:5678
    volumes:
      - n8n_storage:/home/node/.n8n # Persistent storage for workflows and credentials
      - ${SHARED_DIR:-./shared}:/data/shared # Shared directory for workflow data exchange
    secrets:
      - n8n_encryption_key
      - n8n_jwt_secret

  # ==== QDRANT ====
  # Vector database for semantic search and embeddings storage
  # Used by OpenWebUI for RAG (Retrieval-Augmented Generation) capabilities
  qdrant:
    image: qdrant/qdrant:latest
    hostname: qdrant
    container_name: qdrant
    networks: [ 'luminal_default' ]
    restart: unless-stopped
    ports:
      - ${QDRANT_PORT:-6333}:6333 # Vector database API port
    volumes:
      - qdrant_storage:/qdrant/storage # Persistent storage for vector embeddings

  # ==== OPENWEBUI ====
  # Self-hosted AI chat interface with RAG capabilities
  # Provides chat interface for Ollama models with Qdrant vector database integration
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    hostname: openwebui
    networks: [ 'luminal_default' ]
    restart: unless-stopped
    ports:
      - ${OPENWEBUI_PORT:-3000}:8080 # Web UI port (container exposes 8080)
    volumes:
      - openwebui_storage:/app/backend/data # Persistent storage for chat history and settings
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - WEBUI_SECRET_KEY_FILE=/run/secrets/openwebui_secret_key # Secret key from Docker Secrets
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH:-true}
      - RAG_WEB_SEARCH_ENGINE=${RAG_WEB_SEARCH_ENGINE:-searxng}
      - VECTOR_DB=${VECTOR_DB:-qdrant}
      - QDRANT_URI=${QDRANT_URI:-http://qdrant:6333}
    secrets:
      - openwebui_secret_key # Docker Secret for authentication
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # NVIDIA GPU acceleration for AI inference
              count: 1
              capabilities: [ gpu ]
    depends_on:
      - ollama # Wait for Ollama to be ready
      - qdrant # Wait for Qdrant to be ready

  # ==== HOME ASSISTANT ====
  # Self-hosted home automation platform with AI integration
  # Provides device control and automation with n8n workflow integration
  homeassistant:
    image: lscr.io/linuxserver/homeassistant:latest
    container_name: homeassistant
    hostname: homeassistant
    network_mode: host # Host network for automatic device discovery (mDNS/Bonjour)
    restart: unless-stopped
    environment:
      - PUID=${PUID} # Process User ID - Controls file ownership
      - PGID=${PGID} # Process Group ID - Controls file ownership
      - TZ=${TZ} # Timezone setting for accurate scheduling/logs
      - UMASK=${UMASK} # Controls default file permissions
    volumes:
      - homeassistant_storage:/config # Persistent storage for Home Assistant configuration

  # ==== OLLAMA ====
  # Local LLM inference server with NVIDIA GPU acceleration
  # Hosts multiple LLM models: llama3.1:8b, gemma3:12b, gpt-oss:20b
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks: [ 'luminal_default' ]
    restart: unless-stopped
    ports:
      - ${OLLAMA_PORT:-11434}:11434 # Ollama API port for LLM inference
    volumes:
      - ollama_storage:/root/.ollama # Persistent storage for downloaded models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # NVIDIA GPU acceleration for LLM inference
              count: 1
              capabilities: [ gpu ]

  # ==== OLLAMA MODEL PULLERS ====
  # One-time initialization containers to download LLM models
  # These containers run once to pull models, then exit

  # Pull llama3.1:8b model (4.9GB) - Fast general-purpose model
  ollama-pull-llama:
    image: ollama/ollama:latest
    networks: [ 'luminal_default' ]
    container_name: ollama-pull-llama
    volumes:
      - ollama_storage:/root/.ollama # Share storage with main Ollama service
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=${OLLAMA_BASE_URL:-ollama:11434}
    command:
      - "-c"
      - "sleep 3; ollama pull ${OLLAMA_MODEL_LLAMA:-llama3.1:8b}"
    depends_on:
      - ollama

  # Pull gemma3:12b model (8.1GB) - High-performance model for complex tasks
  ollama-pull-gemma:
    image: ollama/ollama:latest
    networks: [ 'luminal_default' ]
    container_name: ollama-pull-gemma
    volumes:
      - ollama_storage:/root/.ollama # Share storage with main Ollama service
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=${OLLAMA_BASE_URL:-ollama:11434}
    command:
      - "-c"
      - "sleep 6; ollama pull ${OLLAMA_MODEL_GEMMA:-gemma3:12b}"
    depends_on:
      - ollama
      - ollama-pull-llama

  # Pull gpt-oss:20b model (~20GB) - Maximum capability for advanced reasoning
  ollama-pull-gpt-oss:
    image: ollama/ollama:latest
    networks: [ 'luminal_default' ]
    container_name: ollama-pull-gpt-oss
    volumes:
      - ollama_storage:/root/.ollama # Share storage with main Ollama service
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=${OLLAMA_BASE_URL:-ollama:11434}
    command:
      - "-c"
      - "sleep 9; ollama pull ${OLLAMA_MODEL_GPT_OSS:-gpt-oss:20b}"
    depends_on:
      - ollama
      - ollama-pull-gemma

  # Pull translategemma:12b model (8.1GB) - Dedicated translation model for 55 languages
  ollama-pull-translategemma:
    image: ollama/ollama:latest
    networks: [ 'luminal_default' ]
    container_name: ollama-pull-translategemma
    volumes:
      - ollama_storage:/root/.ollama # Share storage with main Ollama service
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=${OLLAMA_BASE_URL:-ollama:11434}
    command:
      - "-c"
      - "sleep 12; ollama pull ${OLLAMA_MODEL_TRANSLATEGEMMA:-translategemma:12b}"
    depends_on:
      - ollama
      - ollama-pull-gpt-oss

# Networks for service communication
networks:
  luminal_default:
    external: true # External network for service isolation

# Persistent volumes for service data
volumes:
  n8n_storage:
    external: true
    name: luminal_n8n_storage
  ollama_storage:
    external: true
    name: luminal_ollama_storage
  qdrant_storage:
    external: true
    name: luminal_qdrant_storage
  openwebui_storage:
    external: true
    name: luminal_openwebui_storage
  homeassistant_storage:
    external: true
    name: luminal_homeassistant_storage

# Sensitive information is stored in ./secrets/ and mounted securely
secrets:
  n8n_encryption_key:
    file: ./secrets/n8n_encryption_key.txt # n8n encryption key for secure credential storage
  n8n_jwt_secret:
    file: ./secrets/n8n_jwt_secret.txt # n8n JWT secret for user authentication
  openwebui_secret_key:
    file: ./secrets/openwebui_secret_key.txt # OpenWebUI secret key for session management
